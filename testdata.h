/**
 * @brief Provides test data for Huffman encoding/decoding.
 */

#ifndef TESTDATA_H
#define TESTDATA_H

#define TESTDATA_LEN (7)

const char *test_data[TESTDATA_LEN] = {
    "aaaaaaaaaabbbbbbbbbbccccccccccddddddddddeeeeeeeeeeffffffffffgggggggggghhhhhhhhhhiiiiiiiiiijjjjjjjjjj",

    "zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzxxxyyyyyuvwst",

    "lorem ipsum is simply dummy text of the printing and typesetting industry",

    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc volutpat massa sed hendrerit commodo. Donec volutpat porttitor est a varius. Ut vulputate purus vitae aliquam porta. Nam mattis elementum enim nec porttitor. Proin mattis purus felis. Interdum et malesuada fames ac ante ipsum primis in faucibus. Phasellus et mattis eros. Nam egestas lacus sapien, et condimentum nibh feugiat at. Donec porta eget nisi nec aliquet. Sed ullamcorper diam aliquet, vehicula justo et, efficitur quam. Fusce commodo, nibh a lacinia consequat, neque lacus tincidunt erat, ut mollis turpis felis ac libero. Aenean et tellus a sem pharetra euismod. In hac habitasse platea dictumst. Pellentesque sodales dui sed risus tempor rutrum. Fusce sit amet tincidunt mi. Phasellus vestibulum metus hendrerit, fermentum ligula ut, dignissim magna. Nullam nec vehicula nunc, molestie facilisis elit. Phasellus iaculis suscipit lectus ac elementum. Aliquam ut commodo ex, sed ullamcorper elit. Aenean tincidunt lobortis quam sit amet sagittis. Etiam nisi tellus, scelerisque et enim et, iaculis ultrices ante. Duis sed tellus ac lectus scelerisque volutpat. Maecenas lacus sem, iaculis id congue non, tempus in purus. Integer imperdiet, nunc et molestie dictum, nunc purus tempus tellus, ut consectetur arcu lacus faucibus lacus. Nunc suscipit ultrices egestas. Ut blandit nunc non ullamcorper hendrerit. Duis ultricies sapien risus, id rutrum enim sodales nec. Nulla varius dictum diam quis interdum. Vestibulum quis ante sed ante blandit lobortis eu porttitor velit. Integer vitae nisl consectetur, scelerisque nunc in, porta purus. Vivamus ac dolor vitae lorem dictum pellentesque congue id ipsum. Donec sagittis magna nisi, sit amet laoreet tellus lacinia non. In eleifend neque et laoreet sodales. Quisque at purus semper, cursus elit sed, feugiat ante. Sed consequat mi vel elit vulputate malesuada. Pellentesque accumsan, augue in lobortis porta, purus nibh congue turpis, non rutrum arcu velit et diam. Vivamus accumsan mi turpis, vel placerat neque tristique sed. Duis sodales neque purus, vitae convallis neque bibendum id. Cras mollis ante et nunc scelerisque sagittis. Vivamus sed arcu porttitor, mollis justo sed, porta lacus. Nulla nec accumsan massa. Suspendisse rhoncus egestas ligula, sit amet rhoncus erat rutrum vel. Etiam sit amet purus ultrices, mattis turpis in, sodales sapien. Aenean vitae felis nec leo blandit maximus in sit amet diam. Praesent quis felis a dui porta aliquam. Mauris vulputate mollis vehicula. Suspendisse laoreet feugiat mi, sed aliquam quam egestas id. Nam malesuada risus ut volutpat mattis.",

    "In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression.",

    "In 1951, David A. Huffman and his MIT information theory classmates were given the choice of a term paper or a final exam. The professor, Robert M. Fano, assigned a term paper on the problem of finding the most efficient binary code. Huffman, unable to prove any codes were the most efficient, was about to give up and start studying for the final when he hit upon the idea of using a frequency-sorted binary tree and quickly proved this method the most efficient.[3] In doing so, Huffman outdid Fano, who had worked with information theory inventor Claude Shannon to develop a similar code. By building the tree from the bottom up instead of the top down, Huffman avoided the major flaw of the suboptimal Shannon-Fano coding.",

"The technique works by creating a binary tree of nodes. These can be stored in a regular array, the size of which depends on the number of symbols, n. A node can be either a leaf node or an internal node. Initially, all nodes are leaf nodes, which contain the symbol itself, the weight (frequency of appearance) of the symbol and optionally, a link to a parent node which makes it easy to read the code (in reverse) starting from a leaf node. Internal nodes contain a weight, links to two child nodes and an optional link to a parent node. As a common convention, bit '0' represents following the left child and bit '1' represents following the right child. A finished tree has up to n leaf nodes and n-1 internal nodes. A Huffman tree that omits unused symbols produces the most optimal code lengths. The process begins with the leaf nodes containing the probabilities of the symbol they represent. Then, the process takes the two nodes with smallest probability, and creates a new internal node having these two nodes as children. The weight of the new node is set to the sum of the weight of the children. We then apply the process again, on the new internal node and on the remaining nodes (i.e., we exclude the two leaf nodes), we repeat this process until only one node remains, which is the root of the Huffman tree. The simplest construction algorithm uses a priority queue where the node with lowest probability is given highest priority: Create a leaf node for each symbol and add it to the priority queue. While there is more than one node in the queue: Remove the two nodes of highest priority (lowest probability) from the queue Create a new internal node with these two nodes as children and with probability equal to the sum of the two nodes probabilities. Add the new node to the queue. The remaining node is the root node and the tree is complete. Since efficient priority queue data structures require O(log n) time per insertion, and a tree with n leaves has 2nâˆ’1 nodes, this algorithm operates in O(n log n) time, where n is the number of symbols. If the symbols are sorted by probability, there is a linear-time (O(n)) method to create a Huffman tree using two queues, the first one containing the initial weights (along with pointers to the associated leaves), and combined weights (along with pointers to the trees) being put in the back of the second queue. This assures that the lowest weight is always kept at the front of one of the two queues: Start with as many leaves as there are symbols. Enqueue all leaf nodes into the first queue (by probability in increasing order so that the least likely item is in the head of the queue). While there is more than one node in the queues: Dequeue the two nodes with the lowest weight by examining the fronts of both queues. Create a new internal node, with the two just-removed nodes as children (either node can be either child) and the sum of their weights as the new weight. Enqueue the new node into the rear of the second queue. The remaining node is the root node; the tree has now been generated. Although linear-time given sorted input, in the general case of arbitrary input, using this algorithm requires pre-sorting. Thus, since sorting takes O(n log n) time in the general case, both methods have the same overall complexity. In many cases, time complexity is not very important in the choice of algorithm here, since n here is the number of symbols in the alphabet, which is typically a very small number (compared to the length of the message to be encoded); whereas complexity analysis concerns the behavior when n grows to be very large. It is generally beneficial to minimize the variance of codeword length. For example, a communication buffer receiving Huffman-encoded data may need to be larger to deal with especially long symbols if the tree is especially unbalanced. To minimize variance, simply break ties between queues by choosing the item in the first queue. This modification will retain the mathematical optimality of the Huffman coding while both minimizing variance and minimizing the length of the longest character code."
};

 
#endif
